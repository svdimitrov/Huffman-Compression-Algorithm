{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import heapq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Huffman Compression Algorithm\n",
    "## Author: Stefan Dimitrov"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Abstract\n",
    "This research looks at the work of David Huffman. This project is going to cover knowledge about lossless and lossy compression, the diffrence between them. Also we will see what Huffman trees are, why they are used so much and how well does Huffman compression perform compared to other algorithms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "David Albert Huffman (1925 â€“ 1999) was a pioneer in computer science, known for his Huffman coding. He was also one of the pioneers in the field of mathematical origami. David Huffman died at the age of 74, ten months after being diagnosed with cancer.\n",
    "<img src=\"David Huffman.png\">\n",
    "In computer science and information theory, a Huffman code is a particular type of optimal prefix code that is commonly used for lossless data compression. The process of finding and using such a code proceeds by means of Huffman coding, an algorithm developed by David A. Huffman while he was a Sc.D. student at MIT, and published in the 1952 paper \"A Method for the Construction of Minimum-Redundancy Codes\".\n",
    "\n",
    "The output from Huffman's algorithm can be viewed as a variable-length code table for encoding a source symbol. The algorithm derives this table from the estimated probability or frequency of occurrence for each possible value of the source symbol. As in other entropy encoding methods, more common symbols are generally represented using fewer bits than less common symbols. Huffman's method can be efficiently implemented, finding a code in time linear to the number of input weights if these weights are sorted. However, although optimal among methods encoding symbols separately, Huffman coding is not always optimal among all compression methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is this algorithm used worldwide?\n",
    "In computer science and information theory, Huffman coding is an algorithm used for\n",
    "lossless data compression. Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. Typical examples are executable programs, text documents, and source code. It is also used for image and audio compression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the difference betwenn lossless and lossy compression?\n",
    "### 1. Lossless compression\n",
    "Lossless compression is a class of data compression algorithms that allows the original data to be perfectly reconstructed from the compressed data. By contrast, lossy compression permits reconstruction only of an approximation of the original data, though this usually improves compression rates (and therefore reduces file sizes).\n",
    "\n",
    "Lossless data compression is used in many applications. For example, it is used in the ZIP file format and in the GNU tool gzip. It is also often used as a component within lossy data compression technologies (e.g. lossless mid/side joint stereo preprocessing by MP3 encoders and other lossy audio encoders).\n",
    "\n",
    "Lossless compression is used in cases where it is important that the original and the decompressed data be identical, or where deviations from the original data would be unfavourable. Typical examples are executable programs, text documents, and source code. Some image file formats, like PNG or GIF, use only lossless compression, while others like TIFF and MNG may use either lossless or lossy methods\n",
    "### 2. Lossy compression\n",
    "In information technology, lossy compression or irreversible compression is the class of data encoding methods that uses inexact approximations and partial data discarding to represent the content. These techniques are used to reduce data size for storage, handling, and transmitting content. Different versions of the photo of the cat above show how higher degrees of approximation create coarser images as more details are removed. This is opposed to lossless data compression (reversible data compression) which does not degrade the data. The amount of data reduction possible using lossy compression is much higher than through lossless techniques.\n",
    "\n",
    "Well-designed lossy compression technology often reduces file sizes significantly before degradation is noticed by the end-user. Even when noticeable by the user, further data reduction may be desirable (e.g., for real-time communication, to reduce transmission times, or to reduce storage needs).\n",
    "\n",
    "Lossy compression is most commonly used to compress multimedia data (audio, video, and images), especially in applications such as streaming media and internet telephony. By contrast, lossless compression is typically required for text and data files, such as bank records and text articles. In many cases it is advantageous to make a master lossless file which is to be used to produce new compressed files; for example, a multi-megabyte file can be used at full size to produce a full-page advertisement in a glossy magazine, and a 10 kilobyte lossy copy can be made for a small image on a web page.\n",
    "<img src=\"lossless1.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy encoding\n",
    "Understanding entropy encoding is very important, because one of the most common entropy encoding techniques is exactly Huffman coding (Another really common technique is Arithmetic coding).\n",
    "\n",
    "Entropy is used a lot in science. But for now we are interested only for its use in data compression, computing and information theory.\n",
    "\n",
    "* Data compression - Entropy in data compression may denote the randomness of the data that you are inputing to the compression algorithm. The more the entropy, the lesser the compression ratio. That means the more random the text is, the lesser you can compress it.\n",
    "\n",
    "\n",
    "* Computing - In computing, entropy is the randomness collected by an operating system or application for use in cryptography or other uses that require random data. This randomness is often collected from hardware sources, either pre-existing ones such as mouse movements or specially provided randomness generators.\n",
    "\n",
    "\n",
    "* Information theory - In information theory, entropy is a measure of the uncertainty associated with a random variable. The term by itself in this context usually refers to the Shannon entropy, which quantifies, in the sense of an expected value, the information contained in a message, usually in units such as bits. Equivalently, the Shannon entropy is a measure of the average information content one is missing when one does not know the value of the random variable\n",
    "\n",
    "\n",
    "<img src=\"Entropy.png\">\n",
    "\n",
    "Lets say we have a unfair coin. So do figure out what is the chance do get one of the both sides we have to use this algorithm:\n",
    "### $$H(X) = \\sum_{i=1}^{M} p_ilog_2\\frac{1}{p_i}$$\n",
    "Where 'p' is probability. Lets say that the chances get one of each sides are 0,75 and 0,25. Then:\n",
    "### $$H(X) = 0.75 \\times log_2\\frac{1}{0.75} + 0.25 \\times log_2\\frac{1}{0.25} \\approx 0.811$$\n",
    "<img src=\"Unfair_coin.png\", width=250, height=250>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Huffman trees\n",
    "1. Source symbols\n",
    "2. Frequency table\n",
    "3. Huffman tree\n",
    "4. Huffman code\n",
    "<img src=\"WqamR.png\", width=500, height=500>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### OK, now after we know what a Huffman tree is, we need to learn how to use and print it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printTree(given_tree, depth=0):\n",
    "    value = given_tree[0]\n",
    "    child0 = given_tree[1] if len(given_tree) >= 2 else 'None' # Getting the child if it exists\n",
    "    child1 = given_tree[2] if len(given_tree) >= 3 else 'None'\n",
    "    print(' ' * depth, value) # Printing the value\n",
    "    if child0 != 'None':\n",
    "        printTree(child0, depth + 1)\n",
    "    if child1 != 'None':\n",
    "        printTree(child1, depth + 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " A\n",
      "  B\n",
      "   D\n",
      "  C\n",
      "   E\n",
      "   F\n"
     ]
    }
   ],
   "source": [
    "tree = ['A', ['B', ['D']], ['C', ['E'], ['F']]]\n",
    "printTree(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Now lets try to make a tree from given frequencies:\n",
    "Note that we are using a new import - heapq. It is going to help us making our tree. If you dont know what heapq does check this link: https://docs.python.org/2/library/heapq.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeTree(letterFrequencies):\n",
    "    heap = []\n",
    "    for lf in letterFrequencies:\n",
    "        heapq.heappush(heap, [lf]) # Making our tree with heapq\n",
    "    while len(heap) > 1:\n",
    "        child0 = heapq.heappop(heap) # Gathering all the childs so we can make a node\n",
    "        child1 = heapq.heappop(heap)\n",
    "        freq0, label0 = child0[0]\n",
    "        freq1, label1 = child1[0]\n",
    "        freq = freq0 + freq1\n",
    "        label = ''.join(sorted(label0 + label1))\n",
    "        node = [(freq, label), child0, child1]\n",
    "        heapq.heappush(heap, node) # Finally pushing back the node into our tree\n",
    "    return heap.pop()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(100, 'abcdef'), [(45, 'a')], [(55, 'bcdef'), [(25, 'bc'), [(12, 'c')], [(13, 'b')]], [(30, 'def'), [(14, 'ef'), [(5, 'f')], [(9, 'e')]], [(16, 'd')]]]]\n"
     ]
    }
   ],
   "source": [
    "tree = makeTree([(45, 'a'), (13, 'b'), (12, 'c'), (16, 'd'), (9, 'e'), (5, 'f')])\n",
    "print(tree)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Here is how we can make a code map:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def walkTree(codeTree, codeMap, codePrefix):\n",
    "    if len(codeTree) == 1: # if we reached a leaf we stop\n",
    "        freq, label = codeTree[0]\n",
    "        codeMap[label] = codePrefix\n",
    "    else:\n",
    "        value, child0, child1 = codeTree # if we are still not reahcing a leaf we continue \n",
    "        walkTree(child0, codeMap, codePrefix + '0') # Note that we are using recursion\n",
    "        walkTree(child1, codeMap, codePrefix + '1')\n",
    "\n",
    "\n",
    "def makeCodeMap(codeTree):\n",
    "    codeMap = {}\n",
    "    walkTree(codeTree, codeMap, '')\n",
    "    return codeMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'a': '0', 'c': '100', 'b': '101', 'f': '1100', 'e': '1101', 'd': '111'}\n"
     ]
    }
   ],
   "source": [
    "tree = makeTree([(45, 'a'), (13, 'b'), (12, 'c'), (16, 'd'), (9, 'e'), (5, 'f')])\n",
    "print(makeCodeMap(tree))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Finally the most important functions - encoding and decoding:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(message, frequencies):\n",
    "    codeMap = makeCodeMap(makeTree(frequencies))\n",
    "    return ''.join([codeMap[letter] for letter in message]) #returning a string not a list\n",
    "\n",
    "\n",
    "def decode(encodedMessage, frequencies):\n",
    "    entireTree = makeTree(frequencies)\n",
    "    codeTree = entireTree\n",
    "    decodedLetters = []\n",
    "\n",
    "    for digit in encodedMessage:\n",
    "        if digit == '0': # with this operation we decide to go right or left\n",
    "            codeTree = codeTree[1]\n",
    "        else:\n",
    "            codeTree = codeTree[2]\n",
    "        if len(codeTree) == 1: # if we reached a leaf we stop\n",
    "            frequency, label = codeTree[0]\n",
    "            decodedLetters.append(label)\n",
    "            codeTree = entireTree\n",
    "    return ''.join(decodedLetters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "010101001110110110111000\n",
      "abacdaebfa\n"
     ]
    }
   ],
   "source": [
    "freq = [(45, 'a'), (13, 'b'), (12, 'c'), (16, 'd'), (9, 'e'), (5, 'f')]\n",
    "\n",
    "message = 'abacdaebfa'\n",
    "print(encode(message, freq))\n",
    "\n",
    "encoded_message = '010101001110110110111000'\n",
    "print(decode(encoded_message, freq))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman algorithm compared to LZW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LZW is dictionary-based - as it encodes the input data, it achieves compression by replacing sub-strings that have occurred previously with references into the dictionary.\n",
    "\n",
    "If phrases do not repeat (the data is a stream of symbols in more or less random order), LZW isn't going to be able to compress the data very well.\n",
    "\n",
    "By contrast, Huffman Coding could still compress the data significantly if certain symbols (characters or bytes) occur more frequently than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary:\n",
    "* Lossless and lossy compression.\n",
    "* Entropy encoding.\n",
    "* Huffman tree\n",
    "* Comparison between Huffman compression and LZW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "* https://en.wikipedia.org/wiki/Huffman_coding\n",
    "* https://en.wikipedia.org/wiki/Lossless_compression\n",
    "* https://en.wikipedia.org/wiki/Lossy_compression\n",
    "* https://stackoverflow.com/questions/510412/what-is-the-computer-science-definition-of-entropy\n",
    "* https://www.youtube.com/watch?v=umTbivyJoiI&t=477s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
